{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de las limpiezas previas, se guardó el DataFrame para ejecutar el modelo sin necesidad de repetir el procesamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tipo_propiedad</th>\n",
       "      <th>area_construida</th>\n",
       "      <th>recamaras</th>\n",
       "      <th>banos</th>\n",
       "      <th>edad</th>\n",
       "      <th>estacionamientos</th>\n",
       "      <th>latitud</th>\n",
       "      <th>longitud</th>\n",
       "      <th>Municipio</th>\n",
       "      <th>Colonia</th>\n",
       "      <th>precio_oferta</th>\n",
       "      <th>precio_m2</th>\n",
       "      <th>cp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Departamento</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>19.464082</td>\n",
       "      <td>-99.120232</td>\n",
       "      <td>Gustavo A. Madero</td>\n",
       "      <td>Guadalupe Tepeyac</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>216.216216</td>\n",
       "      <td>07840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Departamento</td>\n",
       "      <td>200.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>19.382607</td>\n",
       "      <td>-99.161064</td>\n",
       "      <td>Benito Juárez</td>\n",
       "      <td>Del Valle Centro</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>275.000000</td>\n",
       "      <td>03100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Departamento</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.508087</td>\n",
       "      <td>-99.136730</td>\n",
       "      <td>Gustavo A. Madero</td>\n",
       "      <td>Sierravista</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>148.648649</td>\n",
       "      <td>07320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Departamento</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.475753</td>\n",
       "      <td>-99.175971</td>\n",
       "      <td>Azcapotzalco</td>\n",
       "      <td>Un Hogar Para Cada Trabajador</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>168.888889</td>\n",
       "      <td>02060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Departamento</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.442391</td>\n",
       "      <td>-99.096380</td>\n",
       "      <td>Venustiano Carranza</td>\n",
       "      <td>Romero Rubio</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>169.230769</td>\n",
       "      <td>15400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tipo_propiedad  area_construida  recamaras  banos  edad  estacionamientos  \\\n",
       "0   Departamento             74.0          3      2     5                 1   \n",
       "1   Departamento            200.0          3      3     7                 3   \n",
       "2   Departamento             74.0          3      1     0                 0   \n",
       "3   Departamento             45.0          1      1     0                 0   \n",
       "4   Departamento             65.0          2      1     0                 0   \n",
       "\n",
       "     latitud   longitud            Municipio                        Colonia  \\\n",
       "0  19.464082 -99.120232    Gustavo A. Madero              Guadalupe Tepeyac   \n",
       "1  19.382607 -99.161064        Benito Juárez               Del Valle Centro   \n",
       "2  19.508087 -99.136730    Gustavo A. Madero                    Sierravista   \n",
       "3  19.475753 -99.175971         Azcapotzalco  Un Hogar Para Cada Trabajador   \n",
       "4  19.442391 -99.096380  Venustiano Carranza                   Romero Rubio   \n",
       "\n",
       "   precio_oferta   precio_m2     cp  \n",
       "0        16000.0  216.216216  07840  \n",
       "1        55000.0  275.000000  03100  \n",
       "2        11000.0  148.648649  07320  \n",
       "3         7600.0  168.888889  02060  \n",
       "4        11000.0  169.230769  15400  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CDMX_test = pd.read_csv('final.csv', dtype={'cp': str}, index_col=\"Unnamed: 0\")\n",
    "df_CDMX_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "12.6\n",
      "tensor([0.3368], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.randn(1).cuda())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.¿Qué algoritmo se puede utilizar como baseline para predecir las variables objetivo? \n",
    "\n",
    "El modelo propuesto predice múltiples objetivos:\n",
    "-Regresión: precio_oferta, precio_m2, latitud, longitud, area_construida, edad, etc.\n",
    "-Clasificación: Municipio, cp.\n",
    "\n",
    "Baselines recomendados:\n",
    "-Para variables continuas: Regresión Lineal Múltiple, Árboles de Decisión, Random Forest o XGBoost (con salidas múltiples).\n",
    "\n",
    "-Para variables categóricas: Logistic Regression (para Municipio/CP) o modelos basados en árboles (Random Forest, LightGBM).\n",
    "\n",
    "-Alternativa unificada: Red Neuronal simple con capas densas y múltiples cabezas de salida.\n",
    "\n",
    "Resultado clave: La pérdida total disminuye de ~1.15 a ~0.79 en 10 épocas, lo que sugiere que el modelo está aprendiendo.\n",
    "Implicación: El modelo propuesto superaría claramente a un baseline naive (ej: predecir promedios),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.¿Se puede determinar la importancia de las características para el modelo generado? \n",
    "\n",
    "La pérdida de especificaciones (specs_loss) es alta (~0.30–0.33), lo que sugiere que el modelo tiene dificultad para predecir variables como area_construida, recamaras, etc.\n",
    "\n",
    "Implicación: Características irrelevantes podrían estar afectando (ej: Colonia si está codificada como ordinal).\n",
    "\n",
    "Solución: Usar SHAP o permutación para identificar y eliminar features redundantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Diagnóstico de Sub/Sobreajuste:\n",
    "-Regularización: Uso de Dropout (Dropout(0.1)), Weight Decay (weight_decay=0.01), y Early Stopping (patience=10).\n",
    "-Pérdida de entrenamiento: Si la pérdida no disminuye, hay subajuste; si cae abruptamente y luego se estanca, posible sobreajuste.\n",
    "\n",
    "Evidencia:\n",
    "-La pérdida de entrenamiento disminuye consistentemente.\n",
    "-La pérdida de consistencia (consistency_loss) se mantiene alta (~0.39–0.41), lo que indica que las predicciones geográficas no se alinean bien con los polígonos de municipios.\n",
    "\n",
    "Diagnóstico:\n",
    "-Posible sobreajuste geográfico: El modelo podría estar memorizando coordenadas en lugar de aprender patrones espaciales reales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Métrica de Negocio Adecuada:\n",
    "Variables clave:\n",
    "-precio_oferta (error directo en pesos) y precio_m2 (indicador de valoración de mercado).\n",
    "\n",
    "Métrica principal:\n",
    "Error Porcentual Absoluto Medio (MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.¿Cuál debería ser el desempeño mínimo a obtener?\n",
    "price_loss (MSE): ~0.25–0.39 → Si el precio promedio es 2.5M MXN, RMSE ≈ √0.3 * 2.5M ≈ 1.37M MXN \n",
    "consistency_loss: ~0.39–0.41 → Indica que las coordenadas predichas están en promedio a 4.1 km fuera del polígono del municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "La GPU está disponible.\n",
      "Datos preprocesados: (113834, 13)\n",
      "Columnas finales: ['area_construida', 'recamaras', 'banos', 'edad', 'estacionamientos', 'latitud', 'longitud', 'precio_oferta', 'precio_m2', 'tipo_propiedad', 'Municipio', 'cp', 'Colonia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emrs9\\AppData\\Local\\Temp\\ipykernel_24940\\2550036240.py:129: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  polygons = gdf.groupby('Municipio')['geometry'].apply(lambda x: x.unary_union.convex_hull)\n",
      "c:\\Users\\emrs9\\.conda\\envs\\trece\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo GeoAwareTabTransformer...\n",
      "Inicio del entrenamiento...\n",
      "Epoch 0 - Total Loss 1.1569 | Price: 0.3911, Geo: 0.0735, Mun: 0.0142, CP: 0.4627, Specs: 0.3346, Consistency: 0.4174\n",
      "Ejemplo de muestras sintéticas:\n",
      "   precio_oferta   precio_m2    latitud   longitud              Municipio  \\\n",
      "0   42045.792344  272.763180  19.433224 -99.183895         Miguel Hidalgo   \n",
      "1   42488.214447  272.567708  19.376882 -99.262916  Cuajimalpa de Morelos   \n",
      "\n",
      "      cp  area_construida  recamaras     banos  edad  estacionamientos  \n",
      "0  04950       165.976135          4  2.103169    18                 1  \n",
      "1  14380       181.264709          2  2.103169    23                 1  \n",
      "Epoch 1 - Total Loss 0.9098 | Price: 0.3246, Geo: 0.0427, Mun: 0.0015, CP: 0.0841, Specs: 0.3175, Consistency: 0.4002\n",
      "Epoch 2 - Total Loss 0.8856 | Price: 0.3202, Geo: 0.0374, Mun: 0.0013, CP: 0.0570, Specs: 0.3131, Consistency: 0.4039\n",
      "Epoch 3 - Total Loss 0.8510 | Price: 0.2938, Geo: 0.0346, Mun: 0.0010, CP: 0.0459, Specs: 0.3121, Consistency: 0.3987\n",
      "Epoch 4 - Total Loss 0.8213 | Price: 0.2695, Geo: 0.0334, Mun: 0.0008, CP: 0.0388, Specs: 0.3105, Consistency: 0.4054\n",
      "Epoch 5 - Total Loss 0.8264 | Price: 0.2791, Geo: 0.0316, Mun: 0.0007, CP: 0.0345, Specs: 0.3097, Consistency: 0.4041\n",
      "Epoch 6 - Total Loss 0.8230 | Price: 0.2789, Geo: 0.0305, Mun: 0.0006, CP: 0.0305, Specs: 0.3094, Consistency: 0.3991\n",
      "Epoch 7 - Total Loss 0.7972 | Price: 0.2546, Geo: 0.0300, Mun: 0.0005, CP: 0.0280, Specs: 0.3089, Consistency: 0.4052\n",
      "Epoch 8 - Total Loss 0.8040 | Price: 0.2649, Geo: 0.0288, Mun: 0.0004, CP: 0.0260, Specs: 0.3082, Consistency: 0.4003\n",
      "Epoch 9 - Total Loss 0.7951 | Price: 0.2575, Geo: 0.0282, Mun: 0.0003, CP: 0.0245, Specs: 0.3078, Consistency: 0.4016\n"
     ]
    }
   ],
   "source": [
    "# Librerías extras a instalar:\n",
    "# pip install geopandas shapely\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Librerías para manejo de geometría\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Escalador personalizado\n",
    "# ---------------------------\n",
    "class GeoScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Escala con (X - mean) / std y permite inverse_transform\"\"\"\n",
    "    def __init__(self):\n",
    "        self.means_ = None\n",
    "        self.stds_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.means_ = X.mean(axis=0)\n",
    "        self.stds_ = X.std(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.means_) / self.stds_\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return X * self.stds_ + self.means_\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Preprocesamiento de datos\n",
    "# ---------------------------\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Verificar columnas requeridas\n",
    "    required_cols = [\n",
    "        'tipo_propiedad', 'area_construida', 'recamaras', 'banos', 'edad',\n",
    "        'estacionamientos', 'latitud', 'longitud', 'Municipio', 'Colonia',\n",
    "        'precio_oferta', 'precio_m2', 'cp'\n",
    "    ]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Falta la columna {col} en tu DataFrame.\")\n",
    "    \n",
    "    # Filtrar CP inválidos y valores faltantes\n",
    "    df = df.dropna(subset=['cp', 'Municipio'])\n",
    "    valid_cp_mask = df['cp'].apply(lambda x: x.isdigit() and len(x) == 5 and x != '00000')\n",
    "    df = df[valid_cp_mask].reset_index(drop=True)\n",
    "    \n",
    "    # Definir columnas:\n",
    "    numerical_features = [\n",
    "        'area_construida', 'recamaras', 'banos', 'edad', 'estacionamientos',\n",
    "        'latitud', 'longitud', 'precio_oferta', 'precio_m2'\n",
    "    ]\n",
    "    categorical_features = ['tipo_propiedad', 'Municipio']  # Se procesan con OrdinalEncoder\n",
    "    # Sólo \"Colonia\" se procesa con OrdinalEncoder; \"cp\" se tratará por separado para usar embeddings.\n",
    "    string_features = ['Colonia']\n",
    "    \n",
    "    # Construir ColumnTransformer para numérico, categórico y Colonia\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', GeoScaler(), numerical_features),\n",
    "            ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, encoded_missing_value=-1), categorical_features),\n",
    "            ('str', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, encoded_missing_value=-1), string_features),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    df_transformed = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # Extraer categorías\n",
    "    cat_encoder = preprocessor.named_transformers_['cat']\n",
    "    str_encoder = preprocessor.named_transformers_['str']\n",
    "    prop_categories = [str(c) for c in cat_encoder.categories_[0]]\n",
    "    mun_categories  = [str(c) for c in cat_encoder.categories_[1]]\n",
    "    col_categories  = [str(c) for c in str_encoder.categories_[0]]\n",
    "    \n",
    "    # Procesar CP usando Pandas para obtener códigos (esto se usará para el embedding)\n",
    "    df['cp_code'] = df['cp'].astype('category').cat.codes\n",
    "    cp_categories = list(df['cp'].astype('category').cat.categories)\n",
    "    \n",
    "    # Armar metadata\n",
    "    metadata = {\n",
    "        'numerical': numerical_features,\n",
    "        'categorical': categorical_features,\n",
    "        'string': string_features,\n",
    "        'preprocessor': preprocessor,\n",
    "        'prop_categories': prop_categories,\n",
    "        'mun_categories': mun_categories,\n",
    "        'cp_categories': cp_categories,\n",
    "        'col_categories': col_categories,\n",
    "        'cp_to_mun': df.groupby('cp')['Municipio'].first().to_dict(),\n",
    "        'cp_freq': df['cp'].value_counts(normalize=True).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Crear DataFrame final: usar columnas transformadas y añadir la columna cp\n",
    "    cols = numerical_features + categorical_features + string_features\n",
    "    df_out = pd.DataFrame(df_transformed, columns=cols)\n",
    "    df_out['cp'] = df['cp_code'].values\n",
    "    # Reordenar: numéricas (0..8), 9: tipo_propiedad, 10: Municipio, 11: cp, 12: Colonia\n",
    "    df_out = df_out[numerical_features + categorical_features + ['cp'] + string_features]\n",
    "    return df_out, metadata\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Funciones para obtener polígonos de municipios\n",
    "# ----------------------------------------------------\n",
    "def compute_municipality_polygons(df):\n",
    "    \"\"\"\n",
    "    Crea un GeoDataFrame y calcula el convex hull para cada municipio.\n",
    "    Retorna un diccionario que mapea el nombre del municipio al polígono resultante.\n",
    "    \"\"\"\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(df['longitud'], df['latitud'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "    polygons = gdf.groupby('Municipio')['geometry'].apply(lambda x: x.unary_union.convex_hull)\n",
    "    return polygons.to_dict()\n",
    "\n",
    "def map_municipality_polygons(polygons, metadata):\n",
    "    \"\"\"\n",
    "    Mapea el diccionario de polígonos (clave: nombre de municipio) a un diccionario\n",
    "    usando como clave el índice según metadata['mun_categories'].\n",
    "    \"\"\"\n",
    "    mun_poly = {}\n",
    "    for idx, mun in enumerate(metadata['mun_categories']):\n",
    "        mun_poly[idx] = polygons.get(mun, None)\n",
    "    return mun_poly\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Clase para ReLU+epsilon\n",
    "# ----------------------------------------------------\n",
    "class ReLUEpsilon(nn.Module):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, x):\n",
    "        return torch.relu(x) + self.epsilon\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 3) Modelo TabTransformer con *todas* las \"heads\"\n",
    "# --------------------------------------------\n",
    "\n",
    "# Decodificador personalizado para \"house specs\"\n",
    "class HouseSpecsDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        # Usar ReLU+epsilon para área, recámaras y baños\n",
    "        self.strict_positive = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 3),\n",
    "            ReLUEpsilon(epsilon=1e-6)\n",
    "        )\n",
    "        # Para edad y estacionamientos, se usa ReLU (pueden ser 0)\n",
    "        self.non_negative = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shared_out = self.shared(x)\n",
    "        out_strict = self.strict_positive(shared_out)\n",
    "        out_nonneg = self.non_negative(shared_out)\n",
    "        # Orden final: [área, recámaras, baños, edad, estacionamientos]\n",
    "        return torch.cat([out_strict, out_nonneg], dim=1)\n",
    "\n",
    "\n",
    "class GeoAwareTabTransformer(nn.Module):\n",
    "    def __init__(self, metadata, embed_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_numerical = len(metadata['numerical'])\n",
    "        \n",
    "        self.prop_vocab_size = len(metadata['prop_categories']) + 1\n",
    "        self.mun_vocab_size  = len(metadata['mun_categories']) + 1\n",
    "        self.cp_vocab_size   = len(metadata['cp_categories']) + 1  # Usamos los CP procesados\n",
    "        \n",
    "        self.prop_embed = nn.Embedding(self.prop_vocab_size, embed_dim, padding_idx=0)\n",
    "        self.mun_embed  = nn.Embedding(self.mun_vocab_size, embed_dim, padding_idx=0)\n",
    "        self.cp_embed   = nn.Embedding(self.cp_vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        self.num_proj = nn.Linear(self.num_numerical, embed_dim * 3)\n",
    "        \n",
    "        self.cat_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim * 3,\n",
    "                nhead=3,\n",
    "                dim_feedforward=256,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.feature_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim * 6,\n",
    "                nhead=6,\n",
    "                dim_feedforward=512,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        self.price_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 6, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 2),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.geo_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        self.mun_classifier = nn.Linear(embed_dim * 3, self.mun_vocab_size)\n",
    "        self.cp_classifier = nn.Linear(embed_dim * 3, self.cp_vocab_size)\n",
    "        self.house_specs_decoder = HouseSpecsDecoder(input_dim=embed_dim * 6, hidden_dim=128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x => [ num(9) | cat(2): (tipo_prop, Municipio) | cp (1) | str(1): (Colonia) ]\n",
    "        Orden: columnas 0..8 = numéricas, 9 = tipo_propiedad, 10 = Municipio, 11 = cp, 12 = Colonia\n",
    "        \"\"\"\n",
    "        x_num = x[:, :9]\n",
    "        prop_indices = torch.clamp(x[:, 9].long() + 1, 0, self.prop_vocab_size - 1)\n",
    "        mun_indices  = torch.clamp(x[:, 10].long() + 1, 0, self.mun_vocab_size - 1)\n",
    "        cp_indices   = torch.clamp(x[:, 11].long() + 1, 0, self.cp_vocab_size - 1)\n",
    "        \n",
    "        prop_emb = self.prop_embed(prop_indices)\n",
    "        mun_emb  = self.mun_embed(mun_indices)\n",
    "        cp_emb   = self.cp_embed(cp_indices)\n",
    "        \n",
    "        cat_emb = torch.cat([prop_emb, mun_emb, cp_emb], dim=1)\n",
    "        cat_context = self.cat_transformer(cat_emb.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        num_proj = self.num_proj(x_num)\n",
    "        combined = torch.cat([cat_context, num_proj], dim=1)\n",
    "        features = self.feature_transformer(combined.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        price_out = self.price_decoder(features)\n",
    "        geo_out   = self.geo_decoder(cat_context)\n",
    "        mun_logits = self.mun_classifier(cat_context)\n",
    "        cp_logits  = self.cp_classifier(cat_context)\n",
    "        house_specs_out = self.house_specs_decoder(features)\n",
    "        \n",
    "        return torch.cat([\n",
    "            price_out,\n",
    "            geo_out,\n",
    "            mun_logits,\n",
    "            cp_logits,\n",
    "            house_specs_out,\n",
    "            features\n",
    "        ], dim=1)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Función de consistency loss basada en proximidad promedio\n",
    "# ----------------------------------------------------\n",
    "def consistency_loss(outputs, metadata):\n",
    "    \"\"\"\n",
    "    Calcula la distancia promedio para las muestras cuya coordenada predicha (geo_out)\n",
    "    se encuentra fuera del polígono correspondiente al municipio predicho.\n",
    "    Si el punto está dentro, se toma 0.\n",
    "    \"\"\"\n",
    "    geo = outputs[:, 2:4]\n",
    "    mun_logits_start = 4\n",
    "    mun_vocab_size = len(metadata['mun_categories']) + 1\n",
    "    mun_logits = outputs[:, mun_logits_start:mun_logits_start + mun_vocab_size]\n",
    "    mun_pred = torch.argmax(mun_logits, dim=1)\n",
    "    \n",
    "    distances = []\n",
    "    n = geo.shape[0]\n",
    "    for i in range(n):\n",
    "        point = Point(geo[i, 0].item(), geo[i, 1].item())\n",
    "        poly = metadata.get('mun_polygons', {}).get(mun_pred[i].item(), None)\n",
    "        if poly is None:\n",
    "            distances.append(100.0)  # Si no hay polígono, asignamos una penalización alta\n",
    "        else:\n",
    "            if poly.contains(point):\n",
    "                distances.append(0.0)\n",
    "            else:\n",
    "                d = poly.exterior.distance(point)\n",
    "                distances.append(d)\n",
    "    avg_distance = np.mean(distances)\n",
    "    avg_distance = (avg_distance - 101) * 10\n",
    "    return torch.tensor(avg_distance, device=outputs.device, dtype=geo.dtype)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Función de pérdida global (GeoLoss)\n",
    "# ----------------------------------------------------\n",
    "class GeoLoss(nn.Module):\n",
    "    def __init__(self, metadata, alpha_mun=0.5, alpha_cp=0.5, alpha_specs=1.5, lambda_consistency=0.1):\n",
    "        \"\"\"\n",
    "        lambda_consistency = peso de la pérdida de consistencia (basada en proximidad promedio)\n",
    "        Se aumenta alpha_specs para incrementar la contribución de house_specs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.cp_to_mun = metadata['cp_to_mun']\n",
    "        self.mun_categories = metadata['mun_categories']\n",
    "        self.cp_categories  = metadata['cp_categories']\n",
    "        \n",
    "        self.mun_vocab_size = len(self.mun_categories) + 1\n",
    "        self.cp_vocab_size  = len(self.cp_categories) + 1\n",
    "        \n",
    "        self.alpha_mun = alpha_mun\n",
    "        self.alpha_cp = alpha_cp\n",
    "        self.alpha_specs = alpha_specs\n",
    "        self.lambda_consistency = lambda_consistency\n",
    "    \n",
    "    def forward(self, outputs, inputs):\n",
    "        valid_mask = (inputs[:, 10] >= 0) & (inputs[:, 11] >= 0)\n",
    "        if not valid_mask.any():\n",
    "            return torch.tensor(0.0, device=outputs.device), {}\n",
    "        \n",
    "        valid_in = inputs[valid_mask]\n",
    "        valid_out = outputs[valid_mask]\n",
    "        \n",
    "        price_loss = self.mse(valid_out[:, :2], valid_in[:, 7:9])\n",
    "        geo_loss = self.mse(valid_out[:, 2:4], valid_in[:, 5:7])\n",
    "        \n",
    "        mun_logits_start = 4\n",
    "        mun_logits_end = 4 + self.mun_vocab_size\n",
    "        mun_logits = valid_out[:, mun_logits_start:mun_logits_end]\n",
    "        cp_indices = valid_in[:, 11].long().cpu().numpy()\n",
    "        cp_list = [self.cp_categories[i] if i < len(self.cp_categories) else '00000' for i in cp_indices]\n",
    "        mun_targets = []\n",
    "        for cp_str in cp_list:\n",
    "            if cp_str in self.cp_to_mun and self.cp_to_mun[cp_str] in self.mun_categories:\n",
    "                idx = self.mun_categories.index(self.cp_to_mun[cp_str])\n",
    "                mun_targets.append(idx)\n",
    "            else:\n",
    "                mun_targets.append(0)\n",
    "        mun_target_tensor = torch.tensor(mun_targets, device=outputs.device)\n",
    "        mun_loss = self.cross_entropy(mun_logits, mun_target_tensor)\n",
    "        \n",
    "        cp_logits_start = mun_logits_end\n",
    "        cp_logits_end = cp_logits_start + self.cp_vocab_size\n",
    "        cp_logits = valid_out[:, cp_logits_start:cp_logits_end]\n",
    "        cp_target_tensor = torch.clamp(valid_in[:, 11].long(), 0, self.cp_vocab_size - 1)\n",
    "        cp_loss = self.cross_entropy(cp_logits, cp_target_tensor)\n",
    "        \n",
    "        house_specs_start = cp_logits_end\n",
    "        house_specs_end = house_specs_start + 5\n",
    "        house_specs_out = valid_out[:, house_specs_start:house_specs_end]\n",
    "        specs_loss = self.mse(house_specs_out, valid_in[:, 0:5])\n",
    "        \n",
    "        cons_loss = consistency_loss(valid_out, metadata)\n",
    "        \n",
    "        total_loss = price_loss + geo_loss + self.alpha_mun * mun_loss + self.alpha_cp * cp_loss + self.alpha_specs * specs_loss + self.lambda_consistency * cons_loss\n",
    "        \n",
    "        losses = {\n",
    "            'price_loss': price_loss,\n",
    "            'geo_loss': geo_loss,\n",
    "            'mun_loss': mun_loss,\n",
    "            'cp_loss': cp_loss,\n",
    "            'specs_loss': specs_loss,\n",
    "            'consistency_loss': cons_loss,\n",
    "            'total_loss': total_loss\n",
    "        }\n",
    "        return total_loss, losses\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Función para generar muestras sintéticas (cada 10 epochs)\n",
    "# ----------------------------------------------------\n",
    "def generate_geo_samples(model, metadata, n_samples=5):\n",
    "    \"\"\"\n",
    "    Genera muestras sintéticas e incluye:\n",
    "      - precio_oferta, precio_m2\n",
    "      - latitud, longitud\n",
    "      - Municipio, cp\n",
    "      - área, recámaras, baños, edad, estacionamientos\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prop_vocab_size = len(metadata['prop_categories']) + 1\n",
    "        mun_vocab_size  = len(metadata['mun_categories']) + 1\n",
    "        cp_vocab_size   = len(metadata['cp_categories']) + 1\n",
    "        \n",
    "        synthetic_in = torch.randn(n_samples, 13, device=device)\n",
    "        synthetic_in[:, 9]  = torch.randint(0, prop_vocab_size, (n_samples,), device=device)\n",
    "        synthetic_in[:, 10] = torch.randint(0, mun_vocab_size, (n_samples,), device=device)\n",
    "        synthetic_in[:, 11] = torch.randint(0, cp_vocab_size, (n_samples,), device=device)\n",
    "        synthetic_in[:, 12] = 0\n",
    "        \n",
    "        out = model(synthetic_in)\n",
    "        out_np = out.cpu().numpy()\n",
    "        \n",
    "        precio_oferta = out_np[:, 0]\n",
    "        precio_m2     = out_np[:, 1]\n",
    "        latitud       = out_np[:, 2]\n",
    "        longitud      = out_np[:, 3]\n",
    "        \n",
    "        mun_logits_start = 4\n",
    "        mun_logits_end = 4 + len(metadata['mun_categories']) + 1\n",
    "        mun_logits = out_np[:, mun_logits_start:mun_logits_end]\n",
    "        mun_pred = np.argmax(mun_logits, axis=1)\n",
    "        \n",
    "        cp_logits_start = mun_logits_end\n",
    "        cp_logits_end = cp_logits_start + len(metadata['cp_categories']) + 1\n",
    "        cp_logits = out_np[:, cp_logits_start:cp_logits_end]\n",
    "        cp_pred = np.argmax(cp_logits, axis=1)\n",
    "        \n",
    "        house_specs_start = cp_logits_end\n",
    "        house_specs_end = house_specs_start + 5\n",
    "        house_specs = out_np[:, house_specs_start:house_specs_end]\n",
    "        \n",
    "        num_scaler = metadata['preprocessor'].named_transformers_['num']\n",
    "        means = pd.Series(num_scaler.means_)\n",
    "        stds  = pd.Series(num_scaler.stds_)\n",
    "        \n",
    "        house_specs_df = pd.DataFrame(house_specs, columns=[\n",
    "            'area_construida','recamaras','banos','edad','estacionamientos'\n",
    "        ])\n",
    "        for i, col in enumerate(house_specs_df.columns):\n",
    "            house_specs_df[col] = house_specs_df[col] * stds.iloc[i] + means.iloc[i]\n",
    "        # Redondear a enteros las variables que deben ser contables:\n",
    "        house_specs_df['recamaras'] = house_specs_df['recamaras'].round(0).astype(int)\n",
    "        house_specs_df['estacionamientos'] = house_specs_df['estacionamientos'].round(0).astype(int)\n",
    "        house_specs_df['edad'] = house_specs_df['edad'].round(0).astype(int)\n",
    "        \n",
    "        latitud = latitud * stds.iloc[5] + means.iloc[5]\n",
    "        longitud = longitud * stds.iloc[6] + means.iloc[6]\n",
    "        precio_oferta = precio_oferta * stds.iloc[7] + means.iloc[7]\n",
    "        precio_m2 = precio_m2 * stds.iloc[8] + means.iloc[8]\n",
    "        \n",
    "        decoded_mun = [\n",
    "            metadata['mun_categories'][idx] if idx < len(metadata['mun_categories'])\n",
    "            else metadata['mun_categories'][0]\n",
    "            for idx in mun_pred\n",
    "        ]\n",
    "        decoded_cp = [\n",
    "            metadata['cp_categories'][idx] if idx < len(metadata['cp_categories'])\n",
    "            else '00000'\n",
    "            for idx in cp_pred\n",
    "        ]\n",
    "        \n",
    "        df_synth = pd.DataFrame({\n",
    "            'precio_oferta': precio_oferta,\n",
    "            'precio_m2': precio_m2,\n",
    "            'latitud': latitud,\n",
    "            'longitud': longitud,\n",
    "            'Municipio': decoded_mun,\n",
    "            'cp': decoded_cp\n",
    "        })\n",
    "        df_synth = pd.concat([df_synth, house_specs_df], axis=1)\n",
    "        return df_synth[[ \n",
    "            'tipo_propiedad' if 'tipo_propiedad' in df_synth.columns else 'precio_oferta',\n",
    "            'precio_m2',\n",
    "            'latitud',\n",
    "            'longitud',\n",
    "            'Municipio',\n",
    "            'cp',\n",
    "            'area_construida',\n",
    "            'recamaras',\n",
    "            'banos',\n",
    "            'edad',\n",
    "            'estacionamientos'\n",
    "        ]] if 'tipo_propiedad' in df_synth.columns else df_synth\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Entrenamiento (con early stopping simple)\n",
    "# ----------------------------------------------------\n",
    "def train_geo_model(df, metadata, epochs=1000, batch_size=64, patience=10):\n",
    "    print(\"Entrenando modelo GeoAwareTabTransformer...\")\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Datos de entrenamiento vacíos tras preprocesamiento.\")\n",
    "    \n",
    "    tensor_data = torch.FloatTensor(df.values).to(device)\n",
    "    dataset = TensorDataset(tensor_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = GeoAwareTabTransformer(metadata).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "    \n",
    "    # Aumentamos alpha_specs para dar mayor peso a los house_specs\n",
    "    criterion = GeoLoss(metadata, alpha_mun=0.7, alpha_cp=0.3, alpha_specs=1.5, lambda_consistency=0.1)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    num_batches = 0\n",
    "    running_losses = {\n",
    "        'price_loss': 0.0,\n",
    "        'geo_loss': 0.0,\n",
    "        'mun_loss': 0.0,\n",
    "        'cp_loss': 0.0,\n",
    "        'specs_loss': 0.0,\n",
    "        'consistency_loss': 0.0\n",
    "    }\n",
    "\n",
    "    print(\"Inicio del entrenamiento...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0].to(device)\n",
    "            valid_mask = (inputs[:, 10] >= 0) & (inputs[:, 11] >= 0)\n",
    "            if not valid_mask.any():\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            valid_inputs = inputs[valid_mask]\n",
    "            loss, losses_dict = criterion(model(valid_inputs), valid_inputs)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            for k in running_losses:\n",
    "                running_losses[k] += losses_dict[k].item()\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        avg_losses = {k: (v / num_batches if num_batches > 0 else 0) for k, v in running_losses.items()}\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if no_improve >= patience:\n",
    "            print(f\"[EARLY STOP] Época {epoch}\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Epoch {epoch} - Total Loss {avg_loss:.4f} | Price: {avg_losses['price_loss']:.4f}, Geo: {avg_losses['geo_loss']:.4f}, Mun: {avg_losses['mun_loss']:.4f}, CP: {avg_losses['cp_loss']:.4f}, Specs: {avg_losses['specs_loss']:.4f}, Consistency: {avg_losses['consistency_loss']:.4f}\")\n",
    "        \n",
    "        # Generar muestras sintéticas cada 10 epochs (y en epoch 0)\n",
    "        if epoch == 0 or epoch % 10 == 0:\n",
    "            try:\n",
    "                samples = generate_geo_samples(model, metadata, n_samples=2)\n",
    "                print(\"Ejemplo de muestras sintéticas:\")\n",
    "                print(samples)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generando samples: {e}\")\n",
    "        \n",
    "        running_losses = {k: 0.0 for k in running_losses}\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Ejemplo de uso\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Asegúrate de tener definido df_CDMX_test con tus datos reales.\n",
    "    # Instala las librerías extras:\n",
    "    # pip install geopandas shapely\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(\"Using device:\", device)\n",
    "        print(\"La GPU está disponible.\")\n",
    "    else:\n",
    "        print(\"La GPU no está disponible.\")\n",
    "\n",
    "    # Suponiendo que df_CDMX_test es tu DataFrame original con tus datos reales\n",
    "    df_processed, metadata = preprocess_data(df_CDMX_test)\n",
    "    print(\"Datos preprocesados:\", df_processed.shape)\n",
    "    print(\"Columnas finales:\", df_processed.columns.tolist())\n",
    "    \n",
    "    # Calcular los polígonos de cada municipio y asignarlos a la metadata\n",
    "    polygons = compute_municipality_polygons(df_CDMX_test)\n",
    "    metadata['mun_polygons'] = map_municipality_polygons(polygons, metadata)\n",
    "    \n",
    "    # Entrenar el modelo\n",
    "    trained_model = train_geo_model(df_processed, metadata, epochs=300, batch_size=2, patience=5)\n",
    "    \n",
    "    # Generar muestras sintéticas finales\n",
    "    synthetic_samples = generate_geo_samples(trained_model, metadata, n_samples=5)\n",
    "    print(\"\\n=== Muestras sintéticas finales ===\")\n",
    "    print(synthetic_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trece",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
